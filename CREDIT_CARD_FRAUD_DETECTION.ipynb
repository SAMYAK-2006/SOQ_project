{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJwrG0JbDFiD"
      },
      "source": [
        "---\n",
        "<h1 align='center' style=\"color:green\"> Credit Card Fraud Detection</h1>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SMke0asDFiF"
      },
      "source": [
        "## 1. Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCJkaekcDFiF"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLoj7USADFiG"
      },
      "source": [
        "### 2. Load The Data Set\n",
        "In the following cells, we will import our dataset from a .csv file as a Pandas DataFrame.  Furthermore, we will begin exploring the dataset to gain an understanding of the type, quantity, and distribution of data in our dataset.  For this purpose, we will use Pandas' built-in describe feature, as well as parameter histograms and a correlation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWJdZNOWDFiG"
      },
      "source": [
        "## Download the dataset from there <a href=\"https://www.kaggle.com/mlg-ulb/creditcardfraud#creditcard.csv\" >click here </a>  \n",
        "\n",
        "Use : [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLdYzS-HDFiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666edcc8-2222-41fe-b29f-29a1ae00157d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "Path to dataset folder: /kaggle/input/creditcardfraud\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Step 1: Download dataset and get the folder path\n",
        "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "\n",
        "# Step 2: Build full path to the CSV file (usually it's named 'creditcard.csv')\n",
        "csv_path = os.path.join(path, \"creditcard.csv\")\n",
        "\n",
        "# Step 3: Load the CSV into a DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Step 4: Show the top rows\n",
        "print(df.head())\n",
        "\n",
        "print(\"Path to dataset folder:\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXPLORE THE DATA\n",
        "\n",
        "**Print the columns**"
      ],
      "metadata": {
        "id": "OpTDIe03m_sJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utoBx_8hDFiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ac2669-b35d-4b1a-eb50-4df84d2325fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Time            V1            V2            V3            V4  \\\n",
            "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
            "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
            "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
            "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
            "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
            "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
            "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
            "\n",
            "                 V5            V6            V7            V8            V9  \\\n",
            "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
            "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
            "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
            "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
            "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
            "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
            "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
            "\n",
            "       ...           V21           V22           V23           V24  \\\n",
            "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
            "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
            "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
            "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
            "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
            "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
            "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
            "\n",
            "                V25           V26           V27           V28         Amount  \\\n",
            "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
            "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
            "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
            "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
            "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
            "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
            "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
            "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
            "\n",
            "               Class  \n",
            "count  284807.000000  \n",
            "mean        0.001727  \n",
            "std         0.041527  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Write the code to print column names.\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print the shape and description**"
      ],
      "metadata": {
        "id": "t18ktUqknkPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBiQxkIgDFiH"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the data\n",
        "\n",
        "\n",
        "#print description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5jIK2gUDFiH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify the absence of null values**\n",
        "\n",
        "Use `data = data.dropna()`,if any."
      ],
      "metadata": {
        "id": "S89T56nynxzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OV-GqqADFiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "e11e343f-c7b5-4f94-d63d-2de03273f5cc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-2665994839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run, and ensure removal if any.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "data.isna().sum()  # Run, and ensure removal if any."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjv1H56IDFiI"
      },
      "source": [
        "## Data Visualization\n",
        "\n",
        "Plot the histograms of each parameter, using .hist() in Matplotlib, and ultimately plt.show(). Learn more about how to do this, by searching online. Note, this is a straightforward code taking no more than 2 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZSM3B2DFiI"
      },
      "outputs": [],
      "source": [
        "# Plot histograms of each parameter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print the number of**\n",
        "  1. **Fraud cases**\n",
        "  2.**Valid Cases**   **from the dataset**\n",
        "\n",
        "  Do this part by **Boolean Indexing**.\n"
      ],
      "metadata": {
        "id": "zDn7PD6eoAAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSz0JqSaDFiI"
      },
      "outputs": [],
      "source": [
        "# Determine number of 1.fraud cases, 2. valid cases.\n",
        "\n",
        "#code goes here:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot a correlation heatmap using the Seaborn Library(Example)**\n",
        "\n",
        "Search these up.\n",
        "\n",
        "1.   pandas.DataFrame.corr\n",
        "2.   seaborn.heatmap\n",
        "3.   matplotlib.pyplot.figure\n",
        "4.   matplotlib.pyplot.show\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AsnYyxMlopKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX9ViDLPDFiI"
      },
      "outputs": [],
      "source": [
        "## Correlation matrix EXAMPLE\n",
        "corrmat=data.corr()\n",
        "fig=plt.figure(figsize=(36,25))\n",
        "\n",
        "sns.heatmap(corrmat, vmax = .8, square = True,annot=True,cmap=\"coolwarm\",linewidth=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InQLeQckDFiI"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting a DataFrame into Features (X) and Target (Y)\n",
        "\n",
        "In this step, we’ll split the given DataFrame into two parts:\n",
        "\n",
        "Features (X): These are the columns that we’ll use as input to our machine learning model. They represent the independent variables that help predict the target variable.\n",
        "\n",
        "Target (Y): This is the column we want to predict. In our case, it’s the “Class” column.\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "1. Get All Columns: First, let’s get a list of all the columns in the DataFrame. We’ll use this list to filter out the columns we don’t want.\n",
        "2. Filter Columns: Remove any columns that are not relevant for our prediction.\n",
        "\n",
        "In our case, we want to exclude the “Class” column from the features.\n",
        "Store Variables:\n",
        "X: Assign the remaining columns (excluding “Class”) to the variable X. These will be our features.\n",
        "Y: Assign the “Class” column to the variable Y. This will be our target.\n",
        "Print Shapes: Finally, print the shapes of X and Y to verify that everything is set up correctly."
      ],
      "metadata": {
        "id": "RfT146iQPjzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZwX-hdUDFiI"
      },
      "outputs": [],
      "source": [
        "# Get all the columns from the dataFrame  [Note: use .tolist()]\n",
        "\n",
        "\n",
        "# Filter the columns to remove data we do not want (Remove Class columns, because that is our target). store under columns variable.\n",
        "\n",
        "\n",
        "# Store the variable we'll be predicting on. store under target variable.\n",
        "\n",
        "\n",
        "X = data[columns]  # all the columns data there except class\n",
        "Y = data[target]   # only Class columns data there\n",
        "\n",
        "\n",
        "# Print shapes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYyftI-hDFiJ"
      },
      "source": [
        "## Split data into Train and test datset (20% test and 80% train)\n",
        "\n",
        "[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ab8KP5bDFiJ"
      },
      "outputs": [],
      "source": [
        "# Split data into 80% train and 20% test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh1PBGkSDFiJ"
      },
      "outputs": [],
      "source": [
        "#Print shapes of train and test sets of X and Y.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOsiEXAADFiJ"
      },
      "source": [
        "### Feature Scaling (Example)\n",
        "#### Converting different units and magnitude data in one unit.\n",
        "Note that this is general procedure, that is followed. This doesn't make a difference in this dataset, but it is done here as an example. [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gaiUO5YDFiJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train_sc=sc.fit_transform(X_train)  # convert all data into float data type\n",
        "X_test_sc=sc.transform(X_test)\n",
        "X_test_sc.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXiXPHzDFiJ"
      },
      "source": [
        "# 3.Machine Learning Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPicIYdVDFiJ"
      },
      "source": [
        "#### We have clean data to build the Ml model. But which Machine learning algorithm is best for the data we have to find. The output is a categorical format so we will use supervised classification machine learning algorithms.\n",
        "\n",
        "#### To build the best model, we have to train and test the dataset with multiple Machine Learning algorithms then we can find the best ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwMDHdHdDFiJ"
      },
      "source": [
        "#### import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xPIiTvnDFiK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Classifier\n",
        "**[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**\n",
        "\n",
        "**[Read for finding accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**"
      ],
      "metadata": {
        "id": "f_gGi8ahEPGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca_WvFLkDFiQ"
      },
      "outputs": [],
      "source": [
        "# Decision tree Classifier\n",
        "\n",
        "#Write code here:  (TRAIN TREE, AND PRINT ACCURACY SCORE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6R1hECEDFiR"
      },
      "outputs": [],
      "source": [
        "# train with Standard Scalar, (instead of X_train, fit on X_train_sc and X_test_sc, achieve by scaling)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiJ8sFQUDFiR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxO8eXbDFiR"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgkE0ntpDFiR"
      },
      "outputs": [],
      "source": [
        "# Random forest classifier, fit on Xtrain achieved by splitting\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf=RandomForestClassifier(n_estimators=20,criterion=\"entropy\",random_state=5)\n",
        "rf_clf.fit(X_train,y_train)\n",
        "y_pred_rf=rf_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiyHfj92DFiR"
      },
      "outputs": [],
      "source": [
        "# train with Standard Scalar, (instead of X_train, fit on X_train_sc and X_test_sc, achieve by scaling)\n",
        "rf_clf_sc=RandomForestClassifier(n_estimators=20,criterion=\"entropy\",random_state=5)\n",
        "rf_clf_sc.fit(X_train_sc,y_train)\n",
        "y_pred_rf_sc=rf_clf_sc.predict(X_test_sc)\n",
        "accuracy_score(y_test,y_pred_rf_sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acqiwI58DFiR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAjgEyJDFiR"
      },
      "source": [
        "##  AdaBoost Classifier\n",
        "\n",
        "Treat as example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifWd4HGEDFiS"
      },
      "outputs": [],
      "source": [
        "# Adaboost classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "abd_clf=AdaBoostClassifier(DecisionTreeClassifier(criterion=\"entropy\",random_state=20),\n",
        "                                                  n_estimators=200,\n",
        "                                                   learning_rate=0.1,\n",
        "                                                   algorithm=\"SAMME.R\",\n",
        "                                                   random_state=1, )\n",
        "\n",
        "abd_clf.fit(X_train,y_train)\n",
        "y_pred_abd=abd_clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred_abd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y2_X2KqDFiS"
      },
      "outputs": [],
      "source": [
        "# Train with Standard Scalar, fit on X_train_sc achieved by scaling\n",
        "abd_clf_sc=AdaBoostClassifier(DecisionTreeClassifier(criterion=\"entropy\",random_state=20),\n",
        "                             n_estimators=200,\n",
        "                             learning_rate=0.1,\n",
        "                             algorithm=\"SAMME.R\",\n",
        "                             random_state=1,)\n",
        "abd_clf_sc.fit(X_train_sc,y_train)\n",
        "y_pred_abd_sc=abd_clf_sc.predict(X_test_sc)\n",
        "accuracy_score(y_test,y_pred_abd_sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-z9XhY6DFiS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mKvJUP5DFiS"
      },
      "source": [
        "##  XGBoost Classifier\n",
        "\n",
        "[Docs](https://xgboost.readthedocs.io/en/stable/python/python_api.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWi6Mcp9DFiS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Train using XGBoost XGBClassifier. Search it up online, and implement similar to the AdaBoost Classifier above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s764Vd3DFiS"
      },
      "outputs": [],
      "source": [
        "# train with Standard Scalar, (instead of X_train, fit on X_train_sc and X_test_sc, achieve by scaling)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1g09WtDFiT"
      },
      "source": [
        "## Confusion Matrix\n",
        "[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0n3vBTSDFiT"
      },
      "outputs": [],
      "source": [
        "#create confusion matrix under the name \"cm\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EtelOxmDFiT"
      },
      "outputs": [],
      "source": [
        "#plot the matrix using the seaborn library. REFER: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhLuiC2mDFiU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbto4sDODFiU"
      },
      "source": [
        "## Classification report of model\n",
        "\n",
        "Print a classification report: [docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfxfHxiaDFiU"
      },
      "outputs": [],
      "source": [
        "#print classification report:\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
